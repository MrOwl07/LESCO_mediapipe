{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250f311b",
   "metadata": {},
   "source": [
    "# **Documentación del proyecto - PuraSeña**\n",
    "\n",
    "## Tabla de contenidos\n",
    "1. [Introducción](#introducción)\n",
    "2. [Arquitectura del Proyecto](#arquitectura-del-proyecto)\n",
    "3. [Dependencias y Entorno](#dependencias-y-entorno)  \n",
    "   3.1. [Instalación rápida](#instalación-rápida)  \n",
    "   3.2. [MediaPipe Hand Landmarker – Guía de Uso](#mediapipe-hand-landmarker--guía-de-uso)  \n",
    "   3.3. [scikit-learn](#scikit-learn)\n",
    "4. [Flujo de Trabajo Detallado](#flujo-de-trabajo-detallado)  \n",
    "   4.1. [Re-entrenar con datos personalizados](#re-entrenar-con-datos-personalizados)\n",
    "5. [Limitaciones actuales](#limitaciones-actuales)\n",
    "6. [Próximos pasos](#próximos-pasos)\n",
    "7. [Licencia](#Licencia)\n",
    "7. [Referencias](#referencias)\n",
    "8. [Especiales Gracias](#especiales-gracias)\n",
    "\n",
    "---\n",
    "\n",
    "## Introducción\n",
    "El proyecto **PuraSeña** es un prototipo que detecta y reconoce gestos de la Lengua de Señas Costarricense (LESCO) en tiempo real, convirtiéndolos en texto escrito. Combina la extracción de puntos de referencia (landmarks) de la mano mediante MediaPipe con modelos clásicos de machine learning para clasificar señales estáticas (letras del alfabeto) y dinámicas (palabras breves).\n",
    "\n",
    "**Objetivos clave**\n",
    "- **Inclusión:** Facilitar la comunicación entre personas oyentes y la comunidad sorda costarricense.\n",
    "- **Ligero y portátil:** Funciona en CPU y puede adaptarse a web o dispositivos móviles.\n",
    "- **Extensible:** Permite añadir nuevas letras o gestos re-entrenando el modelo con datos adicionales.\n",
    "\n",
    "**Estado actual**\n",
    "- Reconocimiento estable de letras del alfabeto LESCO.\n",
    "- Scripts listos para captura de datos, entrenamiento y predicción en vivo.\n",
    "- Pruebas en escritorio (Python 3.12.7 + Pipenv).\n",
    "\n",
    "---\n",
    "\n",
    "## Arquitectura del Proyecto\n",
    "```\n",
    "LESCO_mediapipe/\n",
    "├── .vscode/                # Configuración de VS Code (opcional)\n",
    "├── Pipfile                 # Dependencias gestionadas con Pipenv\n",
    "├── Pipfile.lock            # Versión exacta de cada paquete\n",
    "├── crear_modelo.py         # Entrenamiento y serialización del modelo\n",
    "├── dataset_hands.py        # Captura de datos y generación de CSV\n",
    "├── datos_letras.csv        # Dataset pre-procesado (landmarks + etiqueta)\n",
    "├── imprimir_palabra.py     # Reconstruye palabras a partir de letras detectadas\n",
    "├── label_encoder2.pkl      # Codificador de etiquetas (letras)\n",
    "├── modelo_letras.pkl       # Modelo de clasificación entrenado\n",
    "├── predecir_en_vivo.py     # Inferencia en tiempo real con webcam\n",
    "└── prueba_final.csv        # CSV de prueba / evaluación\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Dependencias y Entorno\n",
    "\n",
    "Se recomienda **Python 3.12.7** y administración de paquetes con **Pipenv**.\n",
    "\n",
    "```toml\n",
    "[packages]\n",
    "mediapipe = \"*\"\n",
    "opencv-python = \"*\"\n",
    "pandas = \"*\"\n",
    "scikit-learn = \"*\"\n",
    "\n",
    "[dev-packages]\n",
    "\n",
    "[requires]\n",
    "python_version = \"3.12\"\n",
    "python_full_version = \"3.12.7\"\n",
    "```\n",
    "\n",
    "### Instalación rápida\n",
    "```bash\n",
    "# Clonar el repositorio\n",
    "git clone https://github.com/MrOwl07/LESCO_mediapipe.git\n",
    "cd LESCO_mediapipe\n",
    "\n",
    "# Crear entorno\n",
    "pip install pipenv  # si no lo tienes\n",
    "pipenv install      # lee el Pipfile\n",
    "pipenv shell        # activa el entorno\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### MediaPipe Hand Landmarker – Guía de Uso\n",
    "\n",
    "| Concepto               | Detalles relevantes                                                                                                                                  |\n",
    "|------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Entrada**            | Fotogramas BGR (OpenCV) o RGB (NumPy) con manos visibles.                                                                                             |\n",
    "| **Salida**             | Lista de 21 *landmarks* por mano (`x`, `y`, `z`) + `handedness` (probabilidad de ser derecha/izquierda) + *scores*.                                  |\n",
    "| **Parámetros clave**   | `num_hands`: máx. manos detectadas.<br>`min_hand_detection_confidence`: umbral inicial (0 – 1).<br>`min_hand_presence_confidence`: confianza para validar.<br>`min_tracking_confidence`: confianza de seguimiento. |\n",
    "| **Normalización**      | El eje `z` es la distancia normalizada a la cámara ⇒ opcionalmente escálala o elimínala antes del modelo.                                             |\n",
    "| **Rendimiento**        | Para CPU ajusta `num_threads`; con GPU (Android/WebGL) la inferencia es más rápida.                                                                     |\n",
    "\n",
    "**Integración en el proyecto**\n",
    "- `dataset_hands.py` – Inicializa `HandLandmarker` y escribe un CSV con 63 columnas (21 × 3) + `label`.\n",
    "- `predecir_en_vivo.py` – Ejecuta inferencia frame a frame, extrae la mano de mayor confianza y envía el vector de 63 features al clasificador.\n",
    "- `imprimir_palabra.py` – Aplica debouncing (≥ 3 fotogramas con la misma letra) para mejorar la precisión.\n",
    "\n",
    "> **Tip de estabilidad:** Iluminación homogénea y fondo despejado mejoran drásticamente la confiabilidad (> 95 % de detección con `min_hand_detection_confidence≈0.5`).\n",
    "\n",
    "---\n",
    "\n",
    "### scikit‑learn\n",
    "\n",
    "1. **Pre‑procesamiento** – Conversión de landmarks en `NumPy arrays`, normalización y escalado opcional.\n",
    "2. **Codificación de etiquetas** – `LabelEncoder` traduce letras (A, B, C…) a enteros.\n",
    "3. **Modelos probados**  \n",
    "   - `RandomForestClassifier(n_estimators=200)`  \n",
    "   - `SVC(kernel=\"rbf\", C=10, gamma=\"scale\")`\n",
    "4. **Validación** – `train_test_split(test_size=0.2)` y `cross_val_score(cv=5)`.\n",
    "5. **Métricas** – `classification_report`, `confusion_matrix`, `accuracy_score`.\n",
    "6. **Serialización** – `joblib.dump()` para `modelo_letras.pkl` y `label_encoder.pkl`.\n",
    "7. **Actualizaciones** – Re-entrena con CSV ampliado y reemplaza los `.pkl`.\n",
    "\n",
    "---\n",
    "\n",
    "## Flujo de Trabajo Detallado\n",
    "\n",
    "| Paso | Script/Fichero       | Descripción                                                                        |\n",
    "|------|----------------------|------------------------------------------------------------------------------------|\n",
    "| 1    | `dataset_hands.py`   | Captura vídeo, extrae landmarks, crea `datos_letras.csv`.                          |\n",
    "| 2    | `crear_modelo.py`    | Limpia datos, balancea clases, entrena modelo, guarda artefactos.                  |\n",
    "| 3    | `predecir_en_vivo.py`| Carga modelo y codificador, abre webcam, muestra predicciones.                     |\n",
    "| 4    | `imprimir_palabra.py`| Lee salidas, forma palabras en consola/archivo.                                    |\n",
    "\n",
    "### Re-entrenar con datos personalizados\n",
    "1. Añade nuevas filas a `datos_letras.csv` (63 columnas + `label`).  \n",
    "2. Corre `python crear_modelo.py --test-size 0.2`.  \n",
    "3. Reemplaza los archivos `.pkl` generados.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitaciones actuales\n",
    "\n",
    "- **Letras con movimiento:** J, LL, Ñ, RR, Z  \n",
    "- **Palabras dinámicas:** “Hola”, “Gracias”, “Perdón”, verbs como “comer”, “beber”, “ir”  \n",
    "- **Expresiones faciales:** emociones y marcadores gramaticales  \n",
    "\n",
    "---\n",
    "\n",
    "## Próximos pasos\n",
    "\n",
    "- **Optimizar para móviles** — TensorFlow Lite en Android.  \n",
    "- **Interfaz gráfica** — UI web/desktop.  \n",
    "- **Gestos dinámicos** — LSTM/CNN + Optical Flow.  \n",
    "- **Expresiones faciales** — Detector de emociones.  \n",
    "- **Dataset ampliado** — Más muestras y variación.\n",
    "- **Entorno laboral y reuniones virtuales** - La aplicación se enfocará en entornos de trabajo colaborativo y videoconferencias (Zoom, Google Meet, Microsoft Teams, etc.). Cuando una persona sorda utilice LESCO frente a la cámara, todos los participantes podrán ver la traducción en pantalla en tiempo real.\n",
    "\n",
    "---\n",
    "\n",
    "## Licencia\n",
    "\n",
    "Este proyecto se distribuye bajo la Licencia MIT.\n",
    "\n",
    "```text\n",
    "\n",
    "Copyright (c) 2025 [Mariana Lai Sánchez]\n",
    "\n",
    "Todos los derechos reservados.\n",
    "Queda prohibido el uso, copia, modificación o distribución de este código total o parcial sin autorización escrita del autor.\n",
    "\n",
    "Copyright (c) 2025 [Mariana Lai Sánchez]\n",
    "\n",
    "All rights reserved. Any use, copying, modification, or distribution of this code, in whole or in part, is strictly prohibited without the author’s written permission.\n",
    "\n",
    "This code is provided solely for personal and educational purposes. Any commercial use requires the author’s written authorization.\n",
    "```\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "[1] Google, \"MediaPipe Hand Landmarker,\" https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker.\n",
    "\n",
    "[2] Scikit-learn developers, \"Scikit-learn: Machine Learning in Python – Stable Documentation,\" https://scikit-learn.org/stable/.\n",
    "\n",
    "---\n",
    "\n",
    "## Agradecimientos Especiales\n",
    "- Beker Esteban Martínez Arias  \n",
    "- Eliana Mena García  \n",
    "- Keylin Beltrán Hernández  \n",
    "- Graciela Lackwood - Intérprete LESCO  \n",
    "- Andher Ramos  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
